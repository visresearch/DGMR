
teacher:
    arch: vit_giant2
    patch_size: 14
    layerscale: 1.0e-05
    ffn_layer: swiglufused
    block_chunks: 0
    qkv_bias: true
    proj_bias: true
    ffn_bias: true
    num_register_tokens: 0
    interpolate_antialias: false
    interpolate_offset: 0.1

    input_size: 518
    weight: /path/to/dinov2_vitg14_pretrain.pth

student:
    arch: vit_giant2
    patch_size: 14
    layerscale: 1.0e-05
    ffn_layer: swiglufused
    block_chunks: 0
    qkv_bias: true
    proj_bias: true
    ffn_bias: true
    num_register_tokens: 0
    interpolate_antialias: false
    interpolate_offset: 0.1

    input_size: 518
    weight: /path/to/0-39_1536/dinov2_vitg14_pretrain_prune.pth

    block_ids: [[0, 1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19, 20, 21, 22, 23, 24, 25, 26, 27, 28, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39]]
    pruned_hidden_sizes: [1536]

data:
    root: /path/to/ILSVRC2012
    input_size: 224
    min_crop: 0.05
    batch_size_per_gpu: 64
    num_workers: 4
    pin_memory: true
    prefetch_factor: 2

optim:
    loss: mse
    trained_modules: whole
    pure_bf16: true
    target: last_block_sep

    use_lr_scheduler: true
    use_wd_scheduler: true

    weight_decay: 0.04
    weight_decay_end: 0.2

    base_lr: 7.5e-6
    min_lr: 1e-7
    nepochs: 10
    warmup_epochs: 1

    use_ema: true
    ema_decay: 0.997